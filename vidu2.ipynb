{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a5bfe1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# demonstrate simple x^2 function\n",
    "from matplotlib import pyplot\n",
    "# simple function\n",
    "def calculate(x):\n",
    " return x * x\n",
    "# define inputs\n",
    "inputs = [-0.5, -0.4, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "# calculate outputs\n",
    "outputs = [calculate(x) for x in inputs]\n",
    "# plot the result\n",
    "pyplot.plot(inputs, outputs)\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d906f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example of generating random samples from X^2\n",
    "from numpy.random import rand\n",
    "from numpy import hstack\n",
    "from matplotlib import pyplot\n",
    "# generate randoms sample from x^2\n",
    "def generate_samples(n=100):\n",
    "# generate random inputs in [-0.5, 0.5]\n",
    " X1 = rand(n) - 0.5\n",
    "# generate outputs X^2 (quadratic)\n",
    " X2 = X1 * X1\n",
    "# stack arrays\n",
    " X1 = X1.reshape(n, 1)\n",
    " X2 = X2.reshape(n, 1)\n",
    " return hstack((X1, X2))\n",
    "# generate samples\n",
    "data = generate_samples()\n",
    "# plot samples\n",
    "pyplot.scatter(data[:, 0], data[:, 1])\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c29ce65d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 25)                75        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 101\n",
      "Trainable params: 101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "# define the discriminator model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    " \n",
    "# define the standalone discriminator model\n",
    "def define_discriminator(n_inputs=2):\n",
    " model = Sequential()\n",
    " model.add(Dense(25, activation='relu', kernel_initializer='he_uniform', input_dim=n_inputs))\n",
    " model.add(Dense(1, activation='sigmoid'))\n",
    "# compile model\n",
    " model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    " return model\n",
    " \n",
    "# define the discriminator model\n",
    "model = define_discriminator()\n",
    "# summarize the model\n",
    "model.summary()\n",
    "# plot the mode\n",
    "plot_model(model, to_file='discriminator_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aefd7d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate n real samples with class labels\n",
    "def generate_real_samples(n):\n",
    "# generate inputs in [-0.5, 0.5]\n",
    " X1 = rand(n) - 0.5\n",
    "# generate outputs X^2\n",
    " X2 = X1 * X1\n",
    "# stack arrays\n",
    " X1 = X1.reshape(n, 1)\n",
    " X2 = X2.reshape(n, 1)\n",
    " X = hstack((X1, X2))\n",
    "# generate class labels\n",
    " y = ones((n, 1))\n",
    " return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29223d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate n fake samples with class labels\n",
    "def generate_fake_samples(n):\n",
    "# generate inputs in [-1, 1]\n",
    " X1 = -1 + rand(n) * 2\n",
    "# generate outputs in [-1, 1]\n",
    " X2 = -1 + rand(n) * 2\n",
    "# stack arrays\n",
    " X1 = X1.reshape(n, 1)\n",
    " X2 = X2.reshape(n, 1)\n",
    " X = hstack((X1, X2))\n",
    "# generate class labels\n",
    " y = zeros((n, 1))\n",
    " return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c2b1dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the discriminator model\n",
    "def train_discriminator(model, n_epochs=1000, n_batch=128):\n",
    " half_batch = int(n_batch / 2)\n",
    "# run epochs manually\n",
    " for i in range(n_epochs):\n",
    "# generate real examples\n",
    "  X_real, y_real = generate_real_samples(half_batch)\n",
    "# update model\n",
    "  model.train_on_batch(X_real, y_real)\n",
    "# generate fake examples\n",
    "  X_fake, y_fake = generate_fake_samples(half_batch)\n",
    "# update model\n",
    "  model.train_on_batch(X_fake, y_fake)\n",
    "# evaluate the model\n",
    "  _, acc_real = model.evaluate(X_real, y_real, verbose=0)\n",
    "  _, acc_fake = model.evaluate(X_fake, y_fake, verbose=0)\n",
    "  print(i, acc_real, acc_fake)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e0990b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.234375 0.796875\n",
      "1 0.171875 0.71875\n",
      "2 0.25 0.828125\n",
      "3 0.3125 0.765625\n",
      "4 0.359375 0.8125\n",
      "5 0.28125 0.796875\n",
      "6 0.265625 0.84375\n",
      "7 0.328125 0.796875\n",
      "8 0.21875 0.8125\n",
      "9 0.28125 0.796875\n",
      "10 0.265625 0.875\n",
      "11 0.28125 0.8125\n",
      "12 0.1875 0.859375\n",
      "13 0.265625 0.859375\n",
      "14 0.203125 0.890625\n",
      "15 0.28125 0.890625\n",
      "16 0.234375 0.875\n",
      "17 0.25 0.875\n",
      "18 0.1875 0.84375\n",
      "19 0.265625 0.859375\n",
      "20 0.28125 0.765625\n",
      "21 0.265625 0.921875\n",
      "22 0.234375 0.875\n",
      "23 0.296875 0.890625\n",
      "24 0.296875 0.890625\n",
      "25 0.28125 0.859375\n",
      "26 0.21875 0.953125\n",
      "27 0.28125 0.84375\n",
      "28 0.21875 0.9375\n",
      "29 0.234375 0.90625\n",
      "30 0.3125 0.9375\n",
      "31 0.171875 0.90625\n",
      "32 0.3125 0.875\n",
      "33 0.28125 0.84375\n",
      "34 0.328125 0.890625\n",
      "35 0.25 0.90625\n",
      "36 0.28125 0.90625\n",
      "37 0.34375 0.921875\n",
      "38 0.3125 0.90625\n",
      "39 0.3125 0.90625\n",
      "40 0.328125 0.890625\n",
      "41 0.3125 0.921875\n",
      "42 0.265625 0.90625\n",
      "43 0.234375 0.90625\n",
      "44 0.34375 0.9375\n",
      "45 0.375 0.9375\n",
      "46 0.296875 0.90625\n",
      "47 0.34375 0.859375\n",
      "48 0.4375 0.921875\n",
      "49 0.3125 0.875\n",
      "50 0.46875 0.953125\n",
      "51 0.265625 0.9375\n",
      "52 0.359375 0.953125\n",
      "53 0.34375 0.953125\n",
      "54 0.40625 0.953125\n",
      "55 0.421875 0.921875\n",
      "56 0.34375 0.984375\n",
      "57 0.515625 0.953125\n",
      "58 0.5 0.953125\n",
      "59 0.484375 0.890625\n",
      "60 0.421875 0.984375\n",
      "61 0.453125 0.96875\n",
      "62 0.4375 0.90625\n",
      "63 0.421875 0.9375\n",
      "64 0.4375 0.9375\n",
      "65 0.46875 0.953125\n",
      "66 0.640625 0.9375\n",
      "67 0.453125 0.953125\n",
      "68 0.59375 0.921875\n",
      "69 0.625 0.9375\n",
      "70 0.5 0.890625\n",
      "71 0.703125 0.9375\n",
      "72 0.625 0.921875\n",
      "73 0.546875 0.984375\n",
      "74 0.625 0.953125\n",
      "75 0.5625 0.96875\n",
      "76 0.625 0.96875\n",
      "77 0.65625 0.953125\n",
      "78 0.765625 0.921875\n",
      "79 0.578125 0.984375\n",
      "80 0.671875 0.9375\n",
      "81 0.65625 0.953125\n",
      "82 0.609375 0.984375\n",
      "83 0.671875 0.984375\n",
      "84 0.640625 0.96875\n",
      "85 0.671875 1.0\n",
      "86 0.703125 0.984375\n",
      "87 0.65625 0.984375\n",
      "88 0.78125 0.953125\n",
      "89 0.65625 1.0\n",
      "90 0.609375 0.890625\n",
      "91 0.6875 0.984375\n",
      "92 0.625 0.984375\n",
      "93 0.640625 0.9375\n",
      "94 0.734375 0.953125\n",
      "95 0.609375 1.0\n",
      "96 0.671875 0.9375\n",
      "97 0.640625 0.984375\n",
      "98 0.640625 0.953125\n",
      "99 0.65625 0.96875\n",
      "100 0.734375 0.96875\n",
      "101 0.6875 0.921875\n",
      "102 0.671875 0.96875\n",
      "103 0.640625 0.953125\n",
      "104 0.734375 0.9375\n",
      "105 0.765625 1.0\n",
      "106 0.625 0.921875\n",
      "107 0.71875 0.96875\n",
      "108 0.59375 0.953125\n",
      "109 0.75 0.96875\n",
      "110 0.734375 0.96875\n",
      "111 0.75 0.984375\n",
      "112 0.671875 0.96875\n",
      "113 0.65625 0.953125\n",
      "114 0.609375 0.96875\n",
      "115 0.75 0.96875\n",
      "116 0.75 0.96875\n",
      "117 0.6875 1.0\n",
      "118 0.71875 0.984375\n",
      "119 0.59375 0.9375\n",
      "120 0.71875 0.96875\n",
      "121 0.734375 0.953125\n",
      "122 0.65625 0.96875\n",
      "123 0.8125 0.984375\n",
      "124 0.65625 0.953125\n",
      "125 0.765625 0.984375\n",
      "126 0.84375 0.96875\n",
      "127 0.75 0.984375\n",
      "128 0.8125 0.921875\n",
      "129 0.75 0.921875\n",
      "130 0.6875 0.9375\n",
      "131 0.6875 0.953125\n",
      "132 0.765625 0.96875\n",
      "133 0.78125 0.96875\n",
      "134 0.8125 0.9375\n",
      "135 0.8125 0.96875\n",
      "136 0.703125 0.890625\n",
      "137 0.6875 1.0\n",
      "138 0.765625 0.96875\n",
      "139 0.78125 0.890625\n",
      "140 0.765625 0.96875\n",
      "141 0.6875 0.9375\n",
      "142 0.765625 1.0\n",
      "143 0.71875 0.953125\n",
      "144 0.703125 0.953125\n",
      "145 0.765625 0.984375\n",
      "146 0.859375 0.96875\n",
      "147 0.78125 0.96875\n",
      "148 0.703125 0.921875\n",
      "149 0.796875 0.96875\n",
      "150 0.765625 0.953125\n",
      "151 0.8125 0.96875\n",
      "152 0.734375 0.96875\n",
      "153 0.8125 0.984375\n",
      "154 0.703125 0.953125\n",
      "155 0.765625 0.96875\n",
      "156 0.78125 0.96875\n",
      "157 0.859375 0.96875\n",
      "158 0.890625 0.96875\n",
      "159 0.78125 0.9375\n",
      "160 0.765625 0.953125\n",
      "161 0.875 1.0\n",
      "162 0.765625 0.96875\n",
      "163 0.796875 0.953125\n",
      "164 0.859375 0.984375\n",
      "165 0.734375 0.96875\n",
      "166 0.71875 0.984375\n",
      "167 0.859375 0.921875\n",
      "168 0.796875 0.953125\n",
      "169 0.859375 0.96875\n",
      "170 0.828125 0.96875\n",
      "171 0.875 0.9375\n",
      "172 0.84375 0.953125\n",
      "173 0.859375 0.90625\n",
      "174 0.8125 0.984375\n",
      "175 0.8125 0.921875\n",
      "176 0.703125 0.9375\n",
      "177 0.78125 0.96875\n",
      "178 0.859375 0.9375\n",
      "179 0.765625 0.984375\n",
      "180 0.78125 0.96875\n",
      "181 0.8125 0.96875\n",
      "182 0.71875 0.953125\n",
      "183 0.734375 0.96875\n",
      "184 0.90625 0.96875\n",
      "185 0.75 0.9375\n",
      "186 0.71875 0.9375\n",
      "187 0.90625 0.953125\n",
      "188 0.84375 0.953125\n",
      "189 0.875 0.90625\n",
      "190 0.875 0.984375\n",
      "191 0.78125 0.953125\n",
      "192 0.84375 0.90625\n",
      "193 0.8125 1.0\n",
      "194 0.8125 0.984375\n",
      "195 0.84375 0.9375\n",
      "196 0.890625 0.921875\n",
      "197 0.84375 0.890625\n",
      "198 0.859375 0.90625\n",
      "199 0.859375 0.953125\n",
      "200 0.90625 0.96875\n",
      "201 0.796875 0.921875\n",
      "202 0.859375 0.90625\n",
      "203 0.796875 0.9375\n",
      "204 0.84375 0.9375\n",
      "205 0.828125 0.921875\n",
      "206 0.953125 0.953125\n",
      "207 0.90625 0.9375\n",
      "208 0.828125 0.921875\n",
      "209 0.84375 0.875\n",
      "210 0.921875 0.984375\n",
      "211 0.859375 0.875\n",
      "212 0.875 0.921875\n",
      "213 0.890625 0.890625\n",
      "214 0.890625 1.0\n",
      "215 0.875 0.96875\n",
      "216 0.78125 0.96875\n",
      "217 0.890625 0.96875\n",
      "218 0.90625 0.984375\n",
      "219 0.890625 0.953125\n",
      "220 0.765625 0.921875\n",
      "221 0.9375 0.9375\n",
      "222 0.859375 0.9375\n",
      "223 0.828125 0.921875\n",
      "224 0.921875 0.9375\n",
      "225 0.8125 0.953125\n",
      "226 0.90625 0.953125\n",
      "227 0.890625 0.96875\n",
      "228 0.84375 0.9375\n",
      "229 0.796875 0.90625\n",
      "230 0.90625 0.90625\n",
      "231 0.796875 0.90625\n",
      "232 0.90625 0.953125\n",
      "233 0.9375 0.984375\n",
      "234 0.9375 0.953125\n",
      "235 0.890625 0.890625\n",
      "236 0.953125 1.0\n",
      "237 0.890625 0.984375\n",
      "238 0.90625 0.90625\n",
      "239 0.84375 0.875\n",
      "240 0.890625 0.953125\n",
      "241 0.859375 0.984375\n",
      "242 0.890625 0.90625\n",
      "243 0.9375 0.921875\n",
      "244 0.953125 0.96875\n",
      "245 0.9375 0.890625\n",
      "246 0.875 0.9375\n",
      "247 0.90625 0.953125\n",
      "248 0.796875 0.921875\n",
      "249 0.953125 0.9375\n",
      "250 0.921875 0.921875\n",
      "251 0.890625 0.953125\n",
      "252 0.96875 0.90625\n",
      "253 0.875 0.953125\n",
      "254 0.953125 0.921875\n",
      "255 0.90625 0.921875\n",
      "256 0.921875 0.96875\n",
      "257 0.84375 0.921875\n",
      "258 0.921875 0.921875\n",
      "259 0.9375 0.953125\n",
      "260 0.9375 0.921875\n",
      "261 0.96875 0.9375\n",
      "262 0.9375 0.9375\n",
      "263 0.828125 0.984375\n",
      "264 0.921875 0.96875\n",
      "265 0.859375 0.90625\n",
      "266 0.90625 0.96875\n",
      "267 0.890625 0.953125\n",
      "268 0.859375 0.9375\n",
      "269 0.90625 0.921875\n",
      "270 0.953125 0.890625\n",
      "271 0.90625 0.9375\n",
      "272 0.921875 0.9375\n",
      "273 0.921875 0.84375\n",
      "274 0.84375 0.90625\n",
      "275 0.859375 0.953125\n",
      "276 0.9375 0.90625\n",
      "277 0.90625 0.921875\n",
      "278 0.90625 0.890625\n",
      "279 0.9375 0.953125\n",
      "280 0.921875 0.875\n",
      "281 0.921875 0.875\n",
      "282 0.9375 0.984375\n",
      "283 0.9375 0.953125\n",
      "284 0.9375 0.890625\n",
      "285 0.953125 0.875\n",
      "286 0.921875 0.90625\n",
      "287 0.9375 0.9375\n",
      "288 0.921875 0.9375\n",
      "289 0.921875 0.921875\n",
      "290 0.921875 0.953125\n",
      "291 0.890625 0.953125\n",
      "292 0.921875 0.9375\n",
      "293 0.984375 0.875\n",
      "294 0.921875 0.875\n",
      "295 0.90625 0.84375\n",
      "296 0.921875 0.953125\n",
      "297 0.984375 0.953125\n",
      "298 0.96875 0.84375\n",
      "299 0.953125 0.90625\n",
      "300 0.9375 0.953125\n",
      "301 0.9375 0.90625\n",
      "302 0.9375 0.890625\n",
      "303 0.90625 0.90625\n",
      "304 0.9375 0.90625\n",
      "305 0.984375 0.984375\n",
      "306 0.9375 0.953125\n",
      "307 0.890625 0.890625\n",
      "308 0.9375 0.9375\n",
      "309 0.9375 0.921875\n",
      "310 0.9375 0.96875\n",
      "311 1.0 0.9375\n",
      "312 1.0 0.9375\n",
      "313 0.921875 0.90625\n",
      "314 0.953125 0.9375\n",
      "315 0.90625 0.921875\n",
      "316 0.96875 0.921875\n",
      "317 0.96875 0.890625\n",
      "318 0.96875 0.875\n",
      "319 0.984375 0.921875\n",
      "320 0.9375 0.890625\n",
      "321 0.953125 0.953125\n",
      "322 1.0 0.90625\n",
      "323 0.9375 0.921875\n",
      "324 1.0 0.890625\n",
      "325 0.953125 0.96875\n",
      "326 0.9375 0.9375\n",
      "327 0.984375 0.984375\n",
      "328 0.96875 0.890625\n",
      "329 0.921875 0.828125\n",
      "330 0.96875 0.90625\n",
      "331 0.953125 0.9375\n",
      "332 0.96875 0.84375\n",
      "333 0.96875 0.890625\n",
      "334 1.0 0.90625\n",
      "335 0.96875 0.90625\n",
      "336 0.96875 0.921875\n",
      "337 0.96875 0.859375\n",
      "338 0.96875 0.9375\n",
      "339 0.953125 0.890625\n",
      "340 0.984375 0.875\n",
      "341 0.984375 0.90625\n",
      "342 1.0 0.859375\n",
      "343 0.984375 0.84375\n",
      "344 0.96875 0.921875\n",
      "345 0.984375 0.921875\n",
      "346 0.96875 0.96875\n",
      "347 0.984375 0.890625\n",
      "348 0.953125 0.921875\n",
      "349 0.984375 0.90625\n",
      "350 0.984375 0.890625\n",
      "351 0.984375 0.90625\n",
      "352 0.984375 0.859375\n",
      "353 1.0 0.875\n",
      "354 0.984375 0.90625\n",
      "355 0.96875 0.921875\n",
      "356 0.953125 0.9375\n",
      "357 0.96875 0.953125\n",
      "358 0.984375 0.90625\n",
      "359 0.953125 0.890625\n",
      "360 0.984375 0.859375\n",
      "361 0.96875 0.9375\n",
      "362 0.96875 0.828125\n",
      "363 1.0 0.90625\n",
      "364 0.953125 0.921875\n",
      "365 0.984375 0.90625\n",
      "366 0.96875 0.90625\n",
      "367 1.0 0.90625\n",
      "368 0.953125 0.9375\n",
      "369 0.984375 0.9375\n",
      "370 1.0 0.875\n",
      "371 1.0 0.921875\n",
      "372 1.0 0.9375\n",
      "373 1.0 0.921875\n",
      "374 1.0 0.796875\n",
      "375 0.984375 0.890625\n",
      "376 0.984375 0.9375\n",
      "377 0.96875 0.90625\n",
      "378 1.0 0.890625\n",
      "379 1.0 0.9375\n",
      "380 0.984375 0.90625\n",
      "381 1.0 0.921875\n",
      "382 0.984375 0.828125\n",
      "383 0.984375 0.96875\n",
      "384 0.984375 0.890625\n",
      "385 0.984375 0.984375\n",
      "386 0.984375 0.921875\n",
      "387 0.96875 0.875\n",
      "388 1.0 0.90625\n",
      "389 1.0 0.84375\n",
      "390 0.984375 0.96875\n",
      "391 0.96875 0.921875\n",
      "392 0.96875 0.90625\n",
      "393 0.984375 0.9375\n",
      "394 1.0 0.953125\n",
      "395 0.984375 0.875\n",
      "396 1.0 0.90625\n",
      "397 0.984375 0.9375\n",
      "398 0.984375 0.90625\n",
      "399 1.0 0.9375\n",
      "400 0.984375 0.9375\n",
      "401 0.984375 0.875\n",
      "402 0.96875 0.921875\n",
      "403 0.96875 0.921875\n",
      "404 0.984375 0.921875\n",
      "405 0.96875 0.9375\n",
      "406 1.0 0.921875\n",
      "407 1.0 0.890625\n",
      "408 0.96875 0.921875\n",
      "409 0.984375 0.921875\n",
      "410 0.984375 0.859375\n",
      "411 1.0 0.984375\n",
      "412 0.984375 0.9375\n",
      "413 1.0 0.90625\n",
      "414 1.0 0.96875\n",
      "415 1.0 0.921875\n",
      "416 1.0 0.96875\n",
      "417 1.0 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418 1.0 0.890625\n",
      "419 1.0 0.84375\n",
      "420 1.0 0.84375\n",
      "421 0.96875 0.875\n",
      "422 1.0 0.9375\n",
      "423 1.0 0.984375\n",
      "424 1.0 0.9375\n",
      "425 1.0 0.890625\n",
      "426 1.0 0.90625\n",
      "427 1.0 0.921875\n",
      "428 1.0 0.921875\n",
      "429 1.0 0.84375\n",
      "430 1.0 0.890625\n",
      "431 1.0 0.953125\n",
      "432 1.0 0.90625\n",
      "433 1.0 0.875\n",
      "434 1.0 0.84375\n",
      "435 1.0 0.875\n",
      "436 1.0 0.859375\n",
      "437 1.0 0.875\n",
      "438 1.0 0.921875\n",
      "439 1.0 0.890625\n",
      "440 1.0 0.890625\n",
      "441 1.0 0.859375\n",
      "442 1.0 0.921875\n",
      "443 1.0 0.90625\n",
      "444 1.0 0.921875\n",
      "445 1.0 0.9375\n",
      "446 1.0 0.90625\n",
      "447 1.0 0.953125\n",
      "448 1.0 0.921875\n",
      "449 1.0 0.84375\n",
      "450 1.0 0.84375\n",
      "451 1.0 0.890625\n",
      "452 1.0 0.90625\n",
      "453 1.0 0.84375\n",
      "454 1.0 0.875\n",
      "455 1.0 0.921875\n",
      "456 1.0 0.96875\n",
      "457 1.0 0.984375\n",
      "458 1.0 0.921875\n",
      "459 1.0 0.875\n",
      "460 1.0 0.875\n",
      "461 1.0 0.953125\n",
      "462 1.0 0.84375\n",
      "463 1.0 0.90625\n",
      "464 1.0 0.921875\n",
      "465 1.0 0.828125\n",
      "466 1.0 0.875\n",
      "467 1.0 0.921875\n",
      "468 1.0 0.78125\n",
      "469 1.0 0.890625\n",
      "470 1.0 0.875\n",
      "471 1.0 0.921875\n",
      "472 1.0 0.859375\n",
      "473 1.0 0.890625\n",
      "474 1.0 0.828125\n",
      "475 1.0 0.90625\n",
      "476 1.0 0.9375\n",
      "477 1.0 0.90625\n",
      "478 1.0 0.90625\n",
      "479 1.0 0.890625\n",
      "480 1.0 0.921875\n",
      "481 1.0 0.921875\n",
      "482 1.0 0.875\n",
      "483 1.0 0.890625\n",
      "484 1.0 0.953125\n",
      "485 1.0 0.90625\n",
      "486 1.0 0.96875\n",
      "487 1.0 0.828125\n",
      "488 1.0 0.875\n",
      "489 1.0 0.90625\n",
      "490 1.0 0.84375\n",
      "491 1.0 0.953125\n",
      "492 1.0 0.9375\n",
      "493 1.0 0.90625\n",
      "494 1.0 0.859375\n",
      "495 1.0 0.84375\n",
      "496 1.0 0.890625\n",
      "497 1.0 0.84375\n",
      "498 1.0 0.9375\n",
      "499 1.0 0.984375\n",
      "500 1.0 0.90625\n",
      "501 1.0 0.90625\n",
      "502 1.0 0.90625\n",
      "503 1.0 0.921875\n",
      "504 1.0 0.953125\n",
      "505 1.0 0.890625\n",
      "506 1.0 0.890625\n",
      "507 1.0 0.875\n",
      "508 1.0 0.875\n",
      "509 1.0 0.9375\n",
      "510 1.0 0.890625\n",
      "511 1.0 0.9375\n",
      "512 1.0 0.9375\n",
      "513 1.0 0.921875\n",
      "514 1.0 0.890625\n",
      "515 1.0 0.9375\n",
      "516 1.0 0.9375\n",
      "517 1.0 0.828125\n",
      "518 1.0 0.890625\n",
      "519 1.0 0.921875\n",
      "520 1.0 0.953125\n",
      "521 1.0 0.875\n",
      "522 1.0 0.84375\n",
      "523 1.0 0.890625\n",
      "524 1.0 0.859375\n",
      "525 1.0 0.890625\n",
      "526 1.0 0.859375\n",
      "527 1.0 0.9375\n",
      "528 1.0 1.0\n",
      "529 1.0 0.9375\n",
      "530 1.0 0.9375\n",
      "531 1.0 0.84375\n",
      "532 1.0 0.78125\n",
      "533 1.0 0.828125\n",
      "534 1.0 0.921875\n",
      "535 1.0 0.890625\n",
      "536 1.0 0.875\n",
      "537 1.0 0.875\n",
      "538 1.0 0.90625\n",
      "539 1.0 0.90625\n",
      "540 1.0 0.90625\n",
      "541 1.0 0.9375\n",
      "542 1.0 0.90625\n",
      "543 1.0 0.90625\n",
      "544 1.0 0.890625\n",
      "545 1.0 0.875\n",
      "546 1.0 0.890625\n",
      "547 1.0 0.921875\n",
      "548 1.0 0.90625\n",
      "549 1.0 0.890625\n",
      "550 1.0 0.90625\n",
      "551 1.0 0.828125\n",
      "552 1.0 0.90625\n",
      "553 1.0 0.9375\n",
      "554 1.0 0.875\n",
      "555 1.0 0.84375\n",
      "556 1.0 0.9375\n",
      "557 1.0 0.90625\n",
      "558 1.0 0.875\n",
      "559 1.0 0.921875\n",
      "560 1.0 0.9375\n",
      "561 1.0 0.890625\n",
      "562 1.0 0.890625\n",
      "563 1.0 0.875\n",
      "564 1.0 0.859375\n",
      "565 1.0 0.890625\n",
      "566 1.0 0.875\n",
      "567 1.0 0.90625\n",
      "568 1.0 0.921875\n",
      "569 1.0 0.90625\n",
      "570 1.0 0.890625\n",
      "571 1.0 0.875\n",
      "572 1.0 0.890625\n",
      "573 1.0 0.921875\n",
      "574 1.0 0.921875\n",
      "575 1.0 0.890625\n",
      "576 1.0 0.84375\n",
      "577 1.0 0.890625\n",
      "578 1.0 0.921875\n",
      "579 1.0 0.9375\n",
      "580 1.0 0.921875\n",
      "581 1.0 0.875\n",
      "582 1.0 0.8125\n",
      "583 1.0 0.921875\n",
      "584 1.0 0.890625\n",
      "585 1.0 0.84375\n",
      "586 1.0 0.859375\n",
      "587 1.0 0.90625\n",
      "588 1.0 0.875\n",
      "589 1.0 0.859375\n",
      "590 1.0 0.859375\n",
      "591 1.0 0.921875\n",
      "592 1.0 0.890625\n",
      "593 1.0 0.875\n",
      "594 1.0 0.921875\n",
      "595 1.0 0.890625\n",
      "596 1.0 0.84375\n",
      "597 1.0 0.953125\n",
      "598 1.0 0.875\n",
      "599 1.0 0.890625\n",
      "600 1.0 0.953125\n",
      "601 1.0 0.921875\n",
      "602 1.0 0.9375\n",
      "603 1.0 0.84375\n",
      "604 1.0 0.828125\n",
      "605 1.0 0.875\n",
      "606 1.0 0.859375\n",
      "607 1.0 0.921875\n",
      "608 1.0 0.828125\n",
      "609 1.0 0.859375\n",
      "610 1.0 0.84375\n",
      "611 1.0 0.890625\n",
      "612 1.0 0.890625\n",
      "613 1.0 0.96875\n",
      "614 1.0 0.875\n",
      "615 1.0 0.90625\n",
      "616 1.0 0.9375\n",
      "617 1.0 0.953125\n",
      "618 1.0 0.84375\n",
      "619 1.0 0.90625\n",
      "620 1.0 0.796875\n",
      "621 1.0 0.84375\n",
      "622 1.0 0.953125\n",
      "623 1.0 0.90625\n",
      "624 1.0 0.875\n",
      "625 1.0 0.875\n",
      "626 1.0 0.890625\n",
      "627 1.0 0.90625\n",
      "628 1.0 0.90625\n",
      "629 1.0 0.921875\n",
      "630 1.0 0.90625\n",
      "631 1.0 0.890625\n",
      "632 1.0 0.890625\n",
      "633 1.0 0.859375\n",
      "634 1.0 0.890625\n",
      "635 1.0 0.921875\n",
      "636 1.0 0.84375\n",
      "637 1.0 0.84375\n",
      "638 1.0 0.9375\n",
      "639 1.0 0.9375\n",
      "640 1.0 0.921875\n",
      "641 1.0 0.90625\n",
      "642 1.0 0.875\n",
      "643 1.0 0.859375\n",
      "644 1.0 0.90625\n",
      "645 1.0 0.84375\n",
      "646 1.0 0.921875\n",
      "647 1.0 0.953125\n",
      "648 1.0 0.890625\n",
      "649 1.0 0.875\n",
      "650 1.0 0.953125\n",
      "651 1.0 0.8125\n",
      "652 1.0 0.859375\n",
      "653 1.0 0.84375\n",
      "654 1.0 0.875\n",
      "655 1.0 0.921875\n",
      "656 1.0 0.8125\n",
      "657 1.0 0.84375\n",
      "658 1.0 0.8125\n",
      "659 1.0 0.875\n",
      "660 1.0 0.953125\n",
      "661 1.0 0.875\n",
      "662 1.0 0.875\n",
      "663 1.0 0.890625\n",
      "664 1.0 0.953125\n",
      "665 1.0 0.9375\n",
      "666 1.0 0.875\n",
      "667 1.0 0.890625\n",
      "668 1.0 0.859375\n",
      "669 1.0 0.84375\n",
      "670 1.0 0.953125\n",
      "671 1.0 0.84375\n",
      "672 1.0 0.84375\n",
      "673 1.0 0.828125\n",
      "674 1.0 0.84375\n",
      "675 1.0 0.9375\n",
      "676 1.0 0.890625\n",
      "677 1.0 0.921875\n",
      "678 1.0 0.90625\n",
      "679 1.0 0.890625\n",
      "680 1.0 0.828125\n",
      "681 1.0 0.859375\n",
      "682 1.0 0.9375\n",
      "683 1.0 0.875\n",
      "684 1.0 0.90625\n",
      "685 1.0 0.875\n",
      "686 1.0 0.875\n",
      "687 1.0 0.828125\n",
      "688 1.0 0.90625\n",
      "689 1.0 0.890625\n",
      "690 1.0 0.90625\n",
      "691 1.0 0.90625\n",
      "692 1.0 0.875\n",
      "693 1.0 0.90625\n",
      "694 1.0 0.9375\n",
      "695 1.0 0.921875\n",
      "696 1.0 0.9375\n",
      "697 1.0 0.890625\n",
      "698 1.0 0.90625\n",
      "699 1.0 0.921875\n",
      "700 1.0 0.90625\n",
      "701 1.0 0.890625\n",
      "702 1.0 0.921875\n",
      "703 1.0 0.90625\n",
      "704 1.0 0.859375\n",
      "705 1.0 0.875\n",
      "706 1.0 0.875\n",
      "707 1.0 0.84375\n",
      "708 1.0 0.859375\n",
      "709 1.0 0.859375\n",
      "710 1.0 0.921875\n",
      "711 1.0 0.921875\n",
      "712 1.0 0.90625\n",
      "713 1.0 0.921875\n",
      "714 1.0 0.9375\n",
      "715 1.0 0.859375\n",
      "716 1.0 0.90625\n",
      "717 1.0 0.921875\n",
      "718 1.0 0.859375\n",
      "719 1.0 0.9375\n",
      "720 1.0 0.90625\n",
      "721 1.0 0.921875\n",
      "722 1.0 0.84375\n",
      "723 1.0 0.921875\n",
      "724 1.0 0.875\n",
      "725 1.0 0.78125\n",
      "726 1.0 0.90625\n",
      "727 1.0 0.859375\n",
      "728 1.0 0.921875\n",
      "729 1.0 0.921875\n",
      "730 1.0 0.875\n",
      "731 1.0 0.921875\n",
      "732 1.0 0.90625\n",
      "733 1.0 0.921875\n",
      "734 1.0 0.8125\n",
      "735 1.0 0.921875\n",
      "736 1.0 0.890625\n",
      "737 1.0 0.875\n",
      "738 1.0 0.875\n",
      "739 1.0 0.828125\n",
      "740 1.0 0.921875\n",
      "741 1.0 0.875\n",
      "742 1.0 0.859375\n",
      "743 1.0 0.875\n",
      "744 1.0 0.875\n",
      "745 1.0 0.828125\n",
      "746 1.0 0.875\n",
      "747 1.0 0.921875\n",
      "748 1.0 0.921875\n",
      "749 1.0 0.828125\n",
      "750 1.0 0.890625\n",
      "751 1.0 0.890625\n",
      "752 1.0 0.875\n",
      "753 1.0 0.921875\n",
      "754 1.0 0.859375\n",
      "755 1.0 0.921875\n",
      "756 1.0 0.8125\n",
      "757 1.0 0.90625\n",
      "758 1.0 0.859375\n",
      "759 1.0 0.859375\n",
      "760 1.0 0.875\n",
      "761 1.0 0.84375\n",
      "762 1.0 0.875\n",
      "763 1.0 0.9375\n",
      "764 1.0 0.796875\n",
      "765 1.0 0.921875\n",
      "766 1.0 0.90625\n",
      "767 1.0 0.890625\n",
      "768 1.0 0.875\n",
      "769 1.0 0.953125\n",
      "770 1.0 0.828125\n",
      "771 1.0 0.84375\n",
      "772 1.0 0.9375\n",
      "773 1.0 0.953125\n",
      "774 1.0 0.84375\n",
      "775 1.0 0.890625\n",
      "776 1.0 0.796875\n",
      "777 1.0 0.890625\n",
      "778 1.0 0.84375\n",
      "779 1.0 0.8125\n",
      "780 1.0 0.921875\n",
      "781 1.0 0.921875\n",
      "782 1.0 0.90625\n",
      "783 1.0 0.90625\n",
      "784 1.0 0.875\n",
      "785 1.0 0.84375\n",
      "786 1.0 0.9375\n",
      "787 1.0 0.859375\n",
      "788 1.0 0.890625\n",
      "789 1.0 0.90625\n",
      "790 1.0 0.875\n",
      "791 1.0 0.875\n",
      "792 1.0 0.90625\n",
      "793 1.0 0.9375\n",
      "794 1.0 0.84375\n",
      "795 1.0 0.875\n",
      "796 1.0 0.9375\n",
      "797 1.0 0.90625\n",
      "798 1.0 0.875\n",
      "799 1.0 0.921875\n",
      "800 1.0 0.84375\n",
      "801 1.0 0.84375\n",
      "802 1.0 0.90625\n",
      "803 1.0 0.875\n",
      "804 1.0 0.90625\n",
      "805 1.0 0.859375\n",
      "806 1.0 0.921875\n",
      "807 1.0 0.921875\n",
      "808 1.0 0.921875\n",
      "809 1.0 0.9375\n",
      "810 1.0 0.875\n",
      "811 1.0 0.890625\n",
      "812 1.0 0.9375\n",
      "813 1.0 0.84375\n",
      "814 1.0 0.921875\n",
      "815 1.0 0.90625\n",
      "816 1.0 0.859375\n",
      "817 1.0 0.921875\n",
      "818 1.0 0.828125\n",
      "819 1.0 0.890625\n",
      "820 1.0 0.9375\n",
      "821 1.0 0.828125\n",
      "822 1.0 0.921875\n",
      "823 1.0 0.921875\n",
      "824 1.0 0.875\n",
      "825 1.0 0.828125\n",
      "826 1.0 0.9375\n",
      "827 1.0 0.9375\n",
      "828 1.0 0.875\n",
      "829 1.0 0.875\n",
      "830 1.0 0.8125\n",
      "831 1.0 0.921875\n",
      "832 1.0 0.90625\n",
      "833 1.0 0.875\n",
      "834 1.0 0.859375\n",
      "835 1.0 0.875\n",
      "836 1.0 0.90625\n",
      "837 1.0 0.890625\n",
      "838 1.0 0.90625\n",
      "839 1.0 0.875\n",
      "840 1.0 0.890625\n",
      "841 1.0 0.859375\n",
      "842 1.0 0.96875\n",
      "843 1.0 0.890625\n",
      "844 1.0 0.8125\n",
      "845 1.0 0.84375\n",
      "846 1.0 0.875\n",
      "847 1.0 0.859375\n",
      "848 1.0 0.890625\n",
      "849 1.0 0.953125\n",
      "850 1.0 0.859375\n",
      "851 1.0 0.859375\n",
      "852 1.0 0.8125\n",
      "853 1.0 0.90625\n",
      "854 1.0 0.828125\n",
      "855 1.0 0.875\n",
      "856 1.0 0.90625\n",
      "857 1.0 0.859375\n",
      "858 1.0 0.859375\n",
      "859 1.0 0.875\n",
      "860 1.0 0.828125\n",
      "861 1.0 0.859375\n",
      "862 1.0 0.890625\n",
      "863 1.0 0.921875\n",
      "864 1.0 0.90625\n",
      "865 1.0 0.90625\n",
      "866 1.0 0.90625\n",
      "867 1.0 0.8125\n",
      "868 1.0 0.90625\n",
      "869 1.0 0.890625\n",
      "870 1.0 0.9375\n",
      "871 1.0 0.90625\n",
      "872 1.0 0.859375\n",
      "873 1.0 0.875\n",
      "874 1.0 0.890625\n",
      "875 1.0 0.890625\n",
      "876 1.0 0.90625\n",
      "877 1.0 0.859375\n",
      "878 1.0 0.921875\n",
      "879 1.0 0.890625\n",
      "880 1.0 0.84375\n",
      "881 1.0 0.859375\n",
      "882 1.0 0.921875\n",
      "883 1.0 0.828125\n",
      "884 1.0 0.90625\n",
      "885 1.0 0.859375\n",
      "886 1.0 0.875\n",
      "887 1.0 0.890625\n",
      "888 1.0 0.921875\n",
      "889 1.0 0.8125\n",
      "890 1.0 0.921875\n",
      "891 1.0 0.890625\n",
      "892 1.0 0.875\n",
      "893 1.0 0.84375\n",
      "894 1.0 0.875\n",
      "895 1.0 0.953125\n",
      "896 1.0 0.890625\n",
      "897 1.0 0.84375\n",
      "898 1.0 0.8125\n",
      "899 1.0 0.9375\n",
      "900 1.0 0.90625\n",
      "901 1.0 0.859375\n",
      "902 1.0 0.9375\n",
      "903 1.0 0.8125\n",
      "904 1.0 0.890625\n",
      "905 1.0 0.84375\n",
      "906 1.0 0.921875\n",
      "907 1.0 0.828125\n",
      "908 1.0 0.875\n",
      "909 1.0 0.890625\n",
      "910 1.0 0.875\n",
      "911 1.0 0.859375\n",
      "912 1.0 0.953125\n",
      "913 1.0 0.90625\n",
      "914 1.0 0.875\n",
      "915 1.0 0.859375\n",
      "916 1.0 0.90625\n",
      "917 1.0 0.828125\n",
      "918 1.0 0.921875\n",
      "919 1.0 0.796875\n",
      "920 1.0 0.90625\n",
      "921 1.0 0.8125\n",
      "922 1.0 0.90625\n",
      "923 1.0 0.921875\n",
      "924 1.0 0.875\n",
      "925 1.0 0.828125\n",
      "926 1.0 0.84375\n",
      "927 1.0 0.875\n",
      "928 1.0 0.859375\n",
      "929 1.0 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "930 1.0 0.90625\n",
      "931 1.0 0.890625\n",
      "932 1.0 0.84375\n",
      "933 1.0 0.828125\n",
      "934 1.0 0.875\n",
      "935 1.0 0.9375\n",
      "936 1.0 0.859375\n",
      "937 1.0 0.890625\n",
      "938 1.0 0.921875\n",
      "939 1.0 0.90625\n",
      "940 1.0 0.828125\n",
      "941 1.0 0.875\n",
      "942 1.0 0.90625\n",
      "943 1.0 0.90625\n",
      "944 1.0 0.890625\n",
      "945 1.0 0.90625\n",
      "946 1.0 0.890625\n",
      "947 1.0 0.796875\n",
      "948 1.0 0.921875\n",
      "949 1.0 0.828125\n",
      "950 1.0 0.828125\n",
      "951 1.0 0.890625\n",
      "952 1.0 0.875\n",
      "953 1.0 0.9375\n",
      "954 1.0 0.8125\n",
      "955 1.0 0.90625\n",
      "956 1.0 0.890625\n",
      "957 1.0 0.875\n",
      "958 1.0 0.9375\n",
      "959 1.0 0.84375\n",
      "960 1.0 0.890625\n",
      "961 1.0 0.921875\n",
      "962 1.0 0.828125\n",
      "963 1.0 0.921875\n",
      "964 1.0 0.890625\n",
      "965 1.0 0.921875\n",
      "966 1.0 0.921875\n",
      "967 1.0 0.875\n",
      "968 1.0 0.890625\n",
      "969 1.0 0.90625\n",
      "970 1.0 0.78125\n",
      "971 1.0 0.890625\n",
      "972 1.0 0.921875\n",
      "973 1.0 0.921875\n",
      "974 1.0 0.9375\n",
      "975 1.0 0.90625\n",
      "976 1.0 0.90625\n",
      "977 1.0 0.890625\n",
      "978 1.0 0.875\n",
      "979 1.0 0.953125\n",
      "980 1.0 0.953125\n",
      "981 1.0 0.921875\n",
      "982 1.0 0.953125\n",
      "983 1.0 0.890625\n",
      "984 1.0 0.875\n",
      "985 1.0 0.90625\n",
      "986 1.0 0.859375\n",
      "987 1.0 0.875\n",
      "988 1.0 0.828125\n",
      "989 1.0 0.859375\n",
      "990 1.0 0.84375\n",
      "991 1.0 0.890625\n",
      "992 1.0 0.875\n",
      "993 1.0 0.875\n",
      "994 1.0 0.796875\n",
      "995 1.0 0.875\n",
      "996 1.0 0.90625\n",
      "997 1.0 0.90625\n",
      "998 1.0 0.9375\n",
      "999 1.0 0.890625\n"
     ]
    }
   ],
   "source": [
    "# define and fit a discriminator model\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import hstack\n",
    "from numpy.random import rand\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# define the standalone discriminator model\n",
    "def define_discriminator(n_inputs=2):\n",
    " model = Sequential()\n",
    " model.add(Dense(25, activation='relu', kernel_initializer='he_uniform',\n",
    " input_dim=n_inputs))\n",
    " model.add(Dense(1, activation='sigmoid'))\n",
    "# compile model\n",
    " model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    " return model\n",
    "# generate n real samples with class labels\n",
    "def generate_real_samples(n):\n",
    "# generate inputs in [-0.5, 0.5]\n",
    " X1 = rand(n) - 0.5\n",
    "# generate outputs X^2\n",
    " X2 = X1 * X1\n",
    "# stack arrays\n",
    " X1 = X1.reshape(n, 1)\n",
    " X2 = X2.reshape(n, 1)\n",
    " X = hstack((X1, X2))\n",
    "# generate class labels\n",
    " y = ones((n, 1))\n",
    " return X, y\n",
    "# generate n fake samples with class labels\n",
    "def generate_fake_samples(n):\n",
    "# generate inputs in [-1, 1]\n",
    " X1 = -1 + rand(n) * 2\n",
    "# generate outputs in [-1, 1]\n",
    " X2 = -1 + rand(n) * 2\n",
    "# stack arrays\n",
    " X1 = X1.reshape(n, 1)\n",
    " X2 = X2.reshape(n, 1)\n",
    " X = hstack((X1, X2))\n",
    "# generate class labels\n",
    " y = zeros((n, 1))\n",
    " return X, y\n",
    "# train the discriminator model\n",
    "def train_discriminator(model, n_epochs=1000, n_batch=128):\n",
    " half_batch = int(n_batch / 2)\n",
    "# run epochs manually\n",
    " for i in range(n_epochs):\n",
    "# generate real examples\n",
    "  X_real, y_real = generate_real_samples(half_batch)\n",
    "# update model\n",
    "  model.train_on_batch(X_real, y_real)\n",
    "# generate fake examples\n",
    "  X_fake, y_fake = generate_fake_samples(half_batch)\n",
    "# update model\n",
    "  model.train_on_batch(X_fake, y_fake)\n",
    "# evaluate the model\n",
    "  _, acc_real = model.evaluate(X_real, y_real, verbose=0)\n",
    "  _, acc_fake = model.evaluate(X_fake, y_fake, verbose=0)\n",
    "  print(i, acc_real, acc_fake)\n",
    "# define the discriminator model\n",
    "model = define_discriminator()\n",
    "# fit the model\n",
    "train_discriminator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "29632991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone generator model\n",
    "def define_generator(latent_dim, n_outputs=2):\n",
    " model = Sequential()\n",
    " model.add(Dense(15, activation='relu', kernel_initializer='he_uniform',\n",
    "  input_dim=latent_dim))\n",
    " model.add(Dense(n_outputs, activation='linear'))\n",
    " return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6b1d988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 15)                90        \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 2)                 32        \n",
      "=================================================================\n",
      "Total params: 122\n",
      "Trainable params: 122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "# define the generator model\n",
    "import tensorflow as tf\n",
    "import pydotplus\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "#define the standalone generator model\n",
    "def define_generator(latent_dim, n_outputs=2):\n",
    " model = Sequential()\n",
    " model.add(Dense(15, activation='relu', kernel_initializer='he_uniform',\n",
    "  input_dim=latent_dim))\n",
    " model.add(Dense(n_outputs, activation='linear'))\n",
    " return model\n",
    "# define the discriminator model\n",
    "model = define_generator(5)\n",
    "# summarize the model\n",
    "model.summary()\n",
    "# plot the model\n",
    "plot_model(model, to_file='generator_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "32fa571f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n):\n",
    "# generate points in the latent space\n",
    " x_input = randn(latent_dim * n)\n",
    "# reshape into a batch of inputs for the network\n",
    " x_input = x_input.reshape(n, latent_dim)\n",
    " return x_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5c6d4a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the generator to generate n fake examples and plot the results\n",
    "def generate_fake_samples(generator, latent_dim, n):\n",
    "# generate points in latent space\n",
    " x_input = generate_latent_points(latent_dim, n)\n",
    "# predict outputs\n",
    " X = generator.predict(x_input)\n",
    "# plot the results\n",
    " pyplot.scatter(X[:, 0], X[:, 1])\n",
    " pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0e4745f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAYT0lEQVR4nO3df6xfd13H8dd7l6vcaMIl2TWyu9XOBIrAyCoXnGlIXBl2MjLKDAKiIfGPKnEGFiy2jgiKhoaqmCgxVlk0ccKWrKuEQQqkIGHJgFva0c2uuPCzd+pKsEHclbXr2z/uvfO77z3n+/2e7/mc8/l8znk+kia9t7fnfL7n+73v8/68Pz+OubsAAPm6LHYDAAD1EMgBIHMEcgDIHIEcADJHIAeAzD0rxkkvv/xy37p1a4xTA0C2jh8//l13Xxj+fpRAvnXrVi0vL8c4NQBky8y+VfR9SisAkDkCOQBkjkAOAJkjkANA5gjkAJC5KLNWcnfkxIoOHj2jx86v6or5Oe3dtU27ty/GbhaAniKQV3TkxIr2Hz6l1QtPSZJWzq9q/+FTkkQwBxAFpZWKDh4983QQ37B64SkdPHomUosA9B2BvKLHzq9W+j4ANI1AXtEV83OVvg8ATSOQV7R31zbNzc4843tzszPau2tbpBYB6DsGOyvaGNBk1gqAVBDIp7B7+yKBG0AyKK0AQOaCBXIzmzGzE2b28VDHBACMFzIjf7uk0wGPBwCYQJBAbmZXSrpJ0t+FOB4AYHKhMvK/kPQuSZfKfsDM9pjZspktnzt3LtBpAQC1A7mZvVbS4+5+fNTPufshd19y96WFhU2PnAMATClERr5D0s1m9k1JH5W008z+McBxAQATqB3I3X2/u1/p7lslvUnSMXf/tdotAwBMhHnkAJC5oCs73f1zkj4X8pgAgNHIyAEgcwRyAMgcgRwAMkcgB4DMEcgBIHMEcgDIHIEcADJHIAeAzPGoN6AlR06s8KxXNIJADrTgyIkV7T98SqsXnpIkrZxf1f7DpySJYI7aKK0ALTh49MzTQXzD6oWndPDomUgtQpcQyIEWPHZ+tdL3gSoI5EALrpifq/R9oAoCOdCCvbu2aW525hnfm5ud0d5d2yK1CF3CYCfQgo0BTWatoAkEcqAlu7cvErjRCEorAJA5AjkAZI5ADgCZI5ADQOYI5ACQOQI5AGSOQA4AmSOQA0DmagdyM3u2mX3JzB40s4fN7A9DNAwAMJkQKzt/KGmnu//AzGYlfcHMPunuDwQ4NgBgjNqB3N1d0g/Wv5xd/+N1jwsAmEyQGrmZzZjZSUmPS/q0u3+x4Gf2mNmymS2fO3cuxGkBAAq0aZa7PyXpWjObl3Svmb3E3R8a+plDkg5J0tLSEhl7T/HcSiC8oLNW3P28pM9JujHkcdENG8+tXDm/Ktf/P7fyyImV2E0DshZi1srCeiYuM5uTdIOkR+oeF93DcyuBZoQorTxP0j+Y2YzWbgx3u/vHAxwXHcNzK4FmhJi18lVJ2wO0BR13xfycVgqCNs+tBOphZSdaw3MrgWbwqDe0hudWAs0gkKNVPLcSCI/SCgBkjkAOAJkjkANA5gjkAJA5AjkAZI5ADgCZI5ADQOaYRw6I7XWRNwI5em9je92NnRk3tteVRDBHFiitoPfYXhe5IyNHoT6VGtheF7kjI8cmfXuST9k2umyvi1wQyLFJ30oNbK+L3FFawSYplhqaLPWwvS5yRyDHJqk9yaeNWSVsrxtWn8ZYUkBpBZukVmroW6knd30bY0kBGTk2Sa3UkGKppy05Zrajbryptz1XBHIUSqnUkFqppy25LlTq8403FkoraM2REyvaceCYrt53n3YcODZxVzu1Us+waV/XOLmWlJjO2T4y8gbk2B1uWp3sMrVSz6Bxr6vOZyHXzHbvrm3PuCZSWjfeLiKQB5Zrd7hpdeumKZV6Bo3Lmut8FnItKaV84+0qAnlgDPQUyzW7HGfU66r7Wcg5s03pxtuHHnLtGrmZXWVmnzWz02b2sJm9PUTDctXVgFVXV+umo15X3c/C7u2Lev8t12hxfk4maXF+Tu+/5ZrOBaEm9WUqZIjBzouS3unuPyPpOkm/bWYvCnDcLHU1YNVVNmB5/QsXGhkobMuogdgQn4Xd2xd1/76d+saBm3T/vp0E8YpyHTCuqnYgd/d/d/evrP/9vyWdltTbT1vqMyxiKcouf/lli7rn+ErW2dKorJnPQnx96SEHrZGb2VZJ2yV9seDf9kjaI0lbtmwJedqkMNBTbrhuuuPAsU6MJ5TVgwc/CyvnVzVj9oxsMKfXmKtcB4yrChbIzezHJd0j6R3u/v3hf3f3Q5IOSdLS0pKHOm+KUhroSVkfsqWNzwEzmeLIecC4iiALgsxsVmtB/E53PxzimOi+vown9KVOW0VTi6iG9WXAuHZGbmYm6cOSTrv7n9dvEvoi52ypypS2PvQ8qmh7rUUfesghMvIdkn5d0k4zO7n+5zUBjouOyzVbqjqlrS89j0nRQwmvdkbu7l+QZAHagh7KMVuqutBnmp5Hlxex0EMJj5WdQEVVA1HVmUxd3+ahLzNJ2kQgByqaJhBV6XlMu7Q/lyw+57GRVLGNLaBqsyiaXugzTekhp6XouY6NpIyMHL1XtZTR9KKvaTL+UJu1tZXV5zg2kjICOXpvmiDYZCCapvQQYgCx67X5LqO0gqjaWhgySmqzKKYpPYSY4si0wHyRkSOaVDLASUoZbQ8kVs34QwwgpnZDw+TIyBFNKhnguMHLHAYSQwwgsnApX2TkiCaVDHDc4GUuT32qW7dnWmC+COSIJqWFIaOCYCo3nKaxBXO+COSIJpcMMKUbTtOYFpgnAjmiySUDrHrDqTswmssKzdT0+boRyBFVDhlglRtO3Zk4qczkKWpXlSDZdlBN9bq1xdzbf1jP0tKSLy8vt35eoGk7DhwrLMMszs/p/n07G///TRgOktJaj6RsVkzVnw8hxevWBDM77u5Lw99n+iGiSGEhUBPqDoymOLBadZpojGmlZddn5fxqZz5boxDI0boc5mVPq+5c7BTncle9ucS4GY26Pl35bI1CIEfrUlkIJIXvGdTdGbHpnRWnUfXmEuNmVHTdNrT52YrV0ySQo3WplA/efeSUbrvrZNCeQd0Vlilu8Vr15hLjZrRx3cq08dmK2dNk1gpal8K87CMnVnTnA9/W8FB/iBWbdWfijPv/MfZ9kSafJhprWunu7Ys6ePRMtM9WzBXABHK0LoWFQAePntkUxDekvGIz1jS7qjenWNNKY362YvY0CeRoXQoLgUb9cqW8YjOXfV9iCf3ZqtL7idnTzCaQ93nVVhfFXghU9ktnUnJbBAxKZXwhZaE+W1V7PzF7A1kMdnZ5uhriKBqQM0lvuW5L0glCitMTu6rq7KqYA9VZZOR0JxFaCuWdaaQwvtAXRT02aXTvJ1ZPM4tATncSTYhd3plGnRsQ5cnJHTmxIpMKB8RT7P0ECeRmdoek10p63N1fEuKYg1KYroY09TE4TXMD6vumUlWVzWpKdQwlVI387yXdGOhYm6S42i1306xAS21/lK6NnTR5fVNaTZuDst6+K80bX5BA7u6fl/S9EMcqkuJqt5xNEwBTDJpdCk5NX1/Kk9WU9fYXE60CtFYjN7M9kvZI0pYtWyr//xzrmamaZvA4xQHnLgWnpq9v2+XJ3EteuQ0qtzb90N0PufuSuy8tLCy0dVoUmCYAphg0uzQVr+nr22Z5MsXeW5FRpazcqgBZzFpBWNNkZykOOOeWNY3S1PUdzIyfMzerZ89epvNPXGg0S06x9zZsksHfqlWAmL2QLBYEIaxpsrMUB5xzy5pGaeL6DmfG51cv6H8vXNIH33it7t+3s7HrlGLvbVjo8ZXYvZBQ0w8/IukXJF1uZmclvcfdPxzi2AhvmrnIqS6g6crYSdH1vf6FCzp49Ixuu+vkVNc7VmacYu9tWOibTexeSJBA7u5vDnEctGeaANiVoJmqwesbYt53E5nxJOWDHEpeoW82sXshlFbQaanNfZ9UiK5/6MHgScsHOZS8QpayjpxY0WVmhf/WVi+Ewc4OyX3KV2g5r2YMkeGFzoyrlA9S772FKhVufMae8s3rQNvshRDIOyLnoNWU2HXLOkZ1/Se9YVcNVuOOG7t8ENq0N5vB63SZWWEQnzFrtRdCIO+InINWU8oCzMr5Ve04cCzpHktZNn39Cxcq3bAnDVaTJAI5DGI2bfg6FQVxSbrk3upnixp5R3QtWwphVIBJdZHKhrI682cfOdfItgST1ORTnILahsFxlnfe/eCm61Sk7ZsbGXlHkC1tVpTVDkq9x1KUTd9218nCn617w54kEQhRV85tHGfSDHxQjJsbgbwjcpjy1bbBwDPNQwJS1NQNe9Lj1hnEzHEcp6inUmTGTJfco92cCOQdkeqCndg2Xv9td53M5iEBozR1w24jEchxHGeSG/3c7MxEA5tN9kYI5B2S+pSvWHJ7SMAo096wxwWRNhKBHMdxynoqVTPwpnsjBHJ03qiHBORoms2cJgkiTScCOY7jlPVUqk4tbLo3QiBH55UFEEnaf/iUlr/1PX32kXOdLUmlUtJoo3wTunwRqqfSdG+EQN4Duc0UCG3U7JXVC0/pzge+/XR2ntIAXKj3LZWSRtPlm3cfOdXIexmip9J0b4RA3nE5zhQIbeN1vqNk6t5wiSWFAbiQ71vMkkbRzej+fTsbOc9gEN+QwnspNd8bYUFQx3XpuZZ17N6+WOl5i7EH4KZ934o2Cau7kGfajcfa3KO7bEBbmvy9bHKDtaY3EiOQd1wq3eoUFAW04j3r4g/ATfO+lQVOSVMHkTrBuM0kYtR1meS9bOOms3v7ou7ft1PfOHBT8Ad7UFrpuBxnCjSl7OEN9xxfSW4h1TTv26jAOW3gqDNQOmqvm6v33Re0Rl52vSadYprKgPC0yMg7rq/7Y5QZzor+ePc1Se6dPc371kTvq84xR910Qme9Zb2tt1y3ZaL3MveeKxl5x7Hic7wUF1JN87410fuqc8xxe91I4bLeup/z3Huu5hNsAhPa0tKSLy8vt35eoMuGZ7pI0y1eCXnMwVkrZZHGJH3jwE1TtS+UJq5dE8zsuLsvDX+fjByILNR88SZ6X9Mcs+z17DhwLNmsN/eeKxk5EFEumWCRooAtqfT1jPq31F9rKsjIgQTlOluibMHSs2cvGzlzRso3600ZgRyIKNfZEmU3oLKBzY3Xk+LAchcQyNFZMfaYqXrOXGZLDL+usk3IyqT2eromyDxyM7vRzM6Y2aNmti/EMYE62lweXuecOczzL3pdZSti5+dmk389XVQ7IzezGUkfkvRqSWclfdnMPubu/1r32OiXkBl0jNrzNOfMYbZE0etyrU0bHJwqMTc7o/fe/OKn/8+o19P3HTlDC1FaeYWkR93965JkZh+V9DpJBHJMLPQujTFqz9OeM/W68agHcyzOzxUG43HTE/u+I2doIQL5oqTvDHx9VtLPBTgueiR0Bh2j9pxLvbuqste1OD831Za0bfSW+pbxh6iRF5XLNk1ON7M9ZrZsZsvnzp0LcFp0SegMOkbtuclzNrnF6jihX1fTvaUY4yOxhQjkZyVdNfD1lZIeG/4hdz/k7kvuvrSwsBDgtOiSsqx12my27v7P0wTOpvacjh2YQr+u0O/1sD7uwV97ZaeZPUvS1yS9StKKpC9L+lV3f7js/7CyE8NSWuGYUlsklS5tn7a0MaztMkTT1/fqffcV7uuSwp4udZWt7Kydkbv7RUm3Sjoq6bSku0cFcaBI009QqSK1jK7JUkSMbL/p97rpjD9FQRYEufsnJH0ixLHQX6nM3khttWWTg6jjBh6bytabfK+bfj5miniwBDAktYxu765tmp155pyC2RkLEphG3bRi1+anlVLvri0s0QeGtJXRVcp2h4u+gTYtHZXt57qhl5RO764tZOTAkDYyuirZ7sGjZ3Th0jMj94VLHqRmXzS1cPYy0xNPXizdTyXVDb1iTtGMjYwcKNB0Rlcl222yZj+8RcBz5mb1P09e1H89caH0/6Q4aNj31aIEckjq30q42KoE56ZXjA7etHYcOKbzq+VBPNVBw5zLQCFQWkG2g1o5qzKg2uYq1VFZfsqDhqnNNGobgRzJzZvugyrBuc1ZGGU3mI3FRykGcSm9mUZto7SC3mczMVTdvrZuzX7S0lmuc7BzbXcoBHJ0dte+1LU1Ra7KQGAO+6MXybXdodTea2Ua7LWSltT2FkFYTe/VgvaU7bVCRo7eZzNdR+ms+wjkkNS/lXB9Qums+wjkQMYmGcRMcSAw9LqFvq+DIJADmZp0EDO10lnoVZiTHK/rgZ7BTiBTuQ5ihm73uON1aTC/sQdLAIgj10HM0O0ed7w+LHgjkAOZynU1Y+h2jzterje8KgjkQCBtb6Pa5h4sIYVu97jj5XrDq4JADgTQxWdfNiV0u8cdL9cbXhUMdgIBbP+jTxXu4Z36wGNfdGXWCis7gYYcObFS+iCGLtVhc9b1BW+UVoCaRs1+6FIdFukikAM1jcq6u1SHRboI5EBNZVn3/Nxsp7vzSAeBHKipbFbEe29+caQWoW9qBXIze4OZPWxml8xs00gq0Ae5TgNEd9SdtfKQpFsk/U2AtgDZ6vqsCKStViB399OSZGZhWgOgsq7Mkcb0WptHbmZ7JO2RpC1btrR1WqDTQm8JizyNrZGb2WfM7KGCP6+rciJ3P+TuS+6+tLCwMH2LATytDzv7YbyxGbm739BGQwBU14ed/YZRStqM6YdAxvqws9+gGJuT5aDu9MPXm9lZST8v6T4zOxqmWQAm0Yed/QZRSipWd9bKvZLuDdQWABWl9jzOpvWxlDQJdj8EMtenOexXzM8VPp+zq6WkSVEjB5CNvpWSJkVGDiAbfSslTYpADiArfSolTYrSCgBkjowcWWNxCEAgR8bYZwRYQ2kF2WJxCLCGQI5ssTgEWEMgR7b6ts8IUIZAjmyxOARYw2AnssXiEGANgRxZY3EIBvV1OiqBHEAn9Hk6KjVyAJ3Q5+moBHIAndDn6agEcgCd0OfpqARyAJ3Q5+moDHYC6IQ+T0clkAPojL5OR6W0AgCZI5ADQOYI5ACQOQI5AGSOQA4AmTN3b/+kZuckfauFU10u6bstnKeOHNoo5dHOHNoo5dHOHNoo9a+dP+XuC8PfjBLI22Jmy+6+FLsdo+TQRimPdubQRimPdubQRol2bqC0AgCZI5ADQOa6HsgPxW7ABHJoo5RHO3Noo5RHO3Noo0Q7JXW8Rg4AfdD1jBwAOo9ADgCZ63wgN7O7zOzk+p9vmtnJ2G0qYma/Y2ZnzOxhM/tA7PYUMbP3mtnKwPV8Tew2lTGz3zUzN7PLY7eliJm9z8y+un4dP2VmV8Ru0zAzO2hmj6y3814zm4/dpiJm9ob135tLZpbUVEQzu3H99/pRM9vX1Hk6H8jd/Y3ufq27XyvpHkmHIzdpEzO7XtLrJL3U3V8s6U8jN2mUD25cT3f/ROzGFDGzqyS9WtK3Y7dlhIPu/tL1z+XHJf1B5PYU+bSkl7j7SyV9TdL+yO0p85CkWyR9PnZDBpnZjKQPSfolSS+S9GYze1ET5+p8IN9gZibpVyR9JHZbCrxN0gF3/6EkufvjkduTuw9KepekZEfy3f37A1/+mBJsq7t/yt0vrn/5gKQrY7anjLufdvcUn7D8CkmPuvvX3f1JSR/VWsIWXG8CuaRXSvpPd/+32A0p8AJJrzSzL5rZv5jZy2M3aIRb17vad5jZc2M3ZpiZ3Sxpxd0fjN2WcczsT8zsO5LeojQz8kG/IemTsRuRmUVJ3xn4+uz694LrxBOCzOwzkn6y4J9ud/d/Xv/7mxUxGx/VRq29D8+VdJ2kl0u628x+2iPMDR3Tzr+W9D6tZY/vk/RnWvsFb9WYNv6+pF9st0XFxn0u3f12Sbeb2X5Jt0p6T6sN1GS/O2Z2u6SLku5ss22DJvwdT40VfK+R3+lOBHJ3v2HUv5vZs7RWQ3tZOy3abFQbzextkg6vB+4vmdklrW2yc66t9m0Ydy03mNnfaq2227qyNprZNZKulvTgWiVNV0r6ipm9wt3/o8UmSpr8Wkr6J0n3KUIgn+B3562SXivpVTESiw0VrmVKzkq6auDrKyU91sSJ+lJauUHSI+5+NnZDShyRtFOSzOwFkn5ECe7oZmbPG/jy9VobZEqGu59y959w963uvlVrv0g/GyOIj2Nmzx/48mZJj8RqSxkzu1HS70m62d2fiN2eDH1Z0vPN7Goz+xFJb5L0sSZO1ImMfAJvUpqDnBvukHSHmT0k6UlJb42Z/YzwATO7Vmvdw29K+s2orcnbATPbJumS1rZ0/q3I7SnyV5J+VNKn13s4D7h7cu00s9dL+ktJC5LuM7OT7r4rcrPk7hfN7FZJRyXNSLrD3R9u4lws0QeAzPWltAIAnUUgB4DMEcgBIHMEcgDIHIEcADJHIAeAzBHIASBz/we2ythLyKSFcQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define and use the generator model\n",
    "import tensorflow as tf\n",
    "from numpy.random import randn\n",
    "from  tensorflow .keras.models import Sequential\n",
    "from  tensorflow .keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "# define the standalone generator model\n",
    "def define_generator(latent_dim, n_outputs=2):\n",
    " model = Sequential()\n",
    " model.add(Dense(15, activation='relu', kernel_initializer='he_uniform',\n",
    "  input_dim=latent_dim))\n",
    " model.add(Dense(n_outputs, activation='linear'))\n",
    " return model\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n):\n",
    "# generate points in the latent space\n",
    " x_input = randn(latent_dim * n)\n",
    "# reshape into a batch of inputs for the network\n",
    " x_input = x_input.reshape(n, latent_dim)\n",
    " return x_input\n",
    "# use the generator to generate n fake examples and plot the results\n",
    "def generate_fake_samples(generator, latent_dim, n):\n",
    "# generate points in latent space\n",
    " x_input = generate_latent_points(latent_dim, n)\n",
    "# predict outputs\n",
    " X = generator.predict(x_input)\n",
    "# plot the results\n",
    " pyplot.scatter(X[:, 0], X[:, 1])\n",
    " pyplot.show()\n",
    "# size of the latent space\n",
    "latent_dim = 5\n",
    "# define the discriminator model\n",
    "model = define_generator(latent_dim)\n",
    "# generate and plot generated samples\n",
    "generate_fake_samples(model, latent_dim, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5871ad32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(generator, discriminator):\n",
    "# make weights in the discriminator not trainable\n",
    " discriminator.trainable = False\n",
    "# connect them\n",
    " model = Sequential()\n",
    "# add generator\n",
    " model.add(generator)\n",
    "# add the discriminator\n",
    " model.add(discriminator)\n",
    "# compile model\n",
    " model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    " return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04be080e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_11 (Sequential)   (None, 2)                 122       \n",
      "_________________________________________________________________\n",
      "sequential_10 (Sequential)   (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 223\n",
      "Trainable params: 122\n",
      "Non-trainable params: 101\n",
      "_________________________________________________________________\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "# demonstrate creating the three models in the gan\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.python.keras.utils.vis_utils  import plot_model\n",
    "from tensorflow.python.keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# define the standalone discriminator model\n",
    "def define_discriminator(n_inputs=2):\n",
    " model = Sequential()\n",
    " model.add(Dense(25, activation='relu', kernel_initializer='he_uniform',\n",
    " input_dim=n_inputs))\n",
    " model.add(Dense(1, activation='sigmoid'))\n",
    "# compile model\n",
    " model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    " return model\n",
    "# define the standalone generator model\n",
    "def define_generator(latent_dim, n_outputs=2):\n",
    " model = Sequential()\n",
    " model.add(Dense(15, activation='relu', kernel_initializer='he_uniform',\n",
    " input_dim=latent_dim))\n",
    " model.add(Dense(n_outputs, activation='linear'))\n",
    " return model\n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(generator, discriminator):\n",
    "# make weights in the discriminator not trainable\n",
    " discriminator.trainable = False\n",
    "# connect them\n",
    " model = Sequential()\n",
    "# add generator\n",
    " model.add(generator)\n",
    "# add the discriminator\n",
    " model.add(discriminator)\n",
    "# compile model\n",
    " model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    " return model\n",
    "# size of the latent space\n",
    "latent_dim = 5\n",
    "# create the discriminator\n",
    "discriminator = define_discriminator()\n",
    "# create the generator\n",
    "generator = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "# summarize gan model\n",
    "gan_model.summary()\n",
    "# plot gan model\n",
    "plot_model(gan_model, to_file='gan_plot.png', show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f91383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the composite model\n",
    "def train_gan(gan_model, latent_dim, n_epochs=10000, n_batch=128):\n",
    "# manually enumerate epochs\n",
    " for i in range(n_epochs):\n",
    "# prepare points in latent space as input for the generator\n",
    "  x_gan = generate_latent_points(latent_dim, n_batch)\n",
    "# create inverted labels for the fake samples\n",
    "  y_gan = ones((n_batch, 1))\n",
    "# update the generator via the discriminators error\n",
    "  gan_model.train_on_batch(x_gan, y_gan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c0e507fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, latent_dim, n_epochs=10000, n_batch=128):\n",
    "# determine half the size of one batch, for updating the discriminator\n",
    " half_batch = int(n_batch / 2)\n",
    "# manually enumerate epochs\n",
    " for i in range(n_epochs):\n",
    "# prepare real samples\n",
    "  x_real, y_real = generate_real_samples(half_batch)\n",
    "# prepare fake examples\n",
    "  x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "# update discriminator\n",
    "  d_model.train_on_batch(x_real, y_real)\n",
    "  d_model.train_on_batch(x_fake, y_fake)\n",
    "# prepare points in latent space as input for the generator\n",
    "  x_gan = generate_latent_points(latent_dim, n_batch)\n",
    "# create inverted labels for the fake samples\n",
    "  y_gan = ones((n_batch, 1))\n",
    "# update the generator via the discriminators error\n",
    "  gan_model.train_on_batch(x_gan, y_gan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "73e7326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate n real samples with class labels\n",
    "def generate_real_samples(n):\n",
    "# generate inputs in [-0.5, 0.5]\n",
    " X1 = rand(n) - 0.5\n",
    "# generate outputs X^2\n",
    " X2 = X1 * X1\n",
    "# stack arrays\n",
    " X1 = X1.reshape(n, 1)\n",
    " X2 = X2.reshape(n, 1)\n",
    " X = hstack((X1, X2))\n",
    "# generate class labels\n",
    " y = ones((n, 1))\n",
    " return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "be415ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n):\n",
    "# generate points in the latent space\n",
    " x_input = randn(latent_dim * n)\n",
    "# reshape into a batch of inputs for the network\n",
    " x_input = x_input.reshape(n, latent_dim)\n",
    " return x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0a9472ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n):\n",
    "# generate points in latent space\n",
    " x_input = generate_latent_points(latent_dim, n)\n",
    "# predict outputs\n",
    " X = generator.predict(x_input)\n",
    "# create class labels\n",
    " y = zeros((n, 1))\n",
    " return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "35f5f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot real and fake points\n",
    "def summarize_performance(generator, latent_dim, n=100):\n",
    "# prepare real samples\n",
    " x_real, y_real = generate_real_samples(n)\n",
    "# prepare fake examples\n",
    " x_fake, y_fake = generate_fake_samples(generator, latent_dim, n)\n",
    "# scatter plot real and fake data points\n",
    " pyplot.scatter(x_real[:, 0], x_real[:, 1], color='red')\n",
    " pyplot.scatter(x_fake[:, 0], x_fake[:, 1], color='blue')\n",
    " pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c682c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the discriminator and plot real and fake points\n",
    "def summarize_performance(epoch, generator, discriminator, latent_dim, n=100):\n",
    "# prepare real samples\n",
    " x_real, y_real = generate_real_samples(n)\n",
    "# evaluate discriminator on real examples\n",
    " _, acc_real = discriminator.evaluate(x_real, y_real, verbose=0)\n",
    "# prepare fake examples\n",
    " x_fake, y_fake = generate_fake_samples(generator, latent_dim, n)\n",
    "# evaluate discriminator on fake examples\n",
    " _, acc_fake = discriminator.evaluate(x_fake, y_fake, verbose=0)\n",
    "# summarize discriminator performance\n",
    " print(epoch, acc_real, acc_fake)\n",
    "# scatter plot real and fake data points\n",
    " pyplot.scatter(x_real[:, 0], x_real[:, 1], color='red')\n",
    " pyplot.scatter(x_fake[:, 0], x_fake[:, 1], color='blue')\n",
    "# save plot to file\n",
    " filename = 'generated_plot_e%03d.png' % (epoch+1)\n",
    " pyplot.savefig(filename)\n",
    " pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dbb6c47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, latent_dim, n_epochs=10000, n_batch=128,\n",
    "n_eval=2000):\n",
    "# determine half the size of one batch, for updating the discriminator\n",
    " half_batch = int(n_batch / 2)\n",
    "# manually enumerate epochs\n",
    " for i in range(n_epochs):\n",
    "# prepare real samples\n",
    "  x_real, y_real = generate_real_samples(half_batch)\n",
    "# prepare fake examples\n",
    "  x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    " # update discriminator\n",
    "  d_model.train_on_batch(x_real, y_real)\n",
    "  d_model.train_on_batch(x_fake, y_fake)\n",
    "# prepare points in latent space as input for the generator\n",
    "  x_gan = generate_latent_points(latent_dim, n_batch)\n",
    "# create inverted labels for the fake samples\n",
    "  y_gan = ones((n_batch, 1))\n",
    "# update the generator via the discriminators error\n",
    "  gan_model.train_on_batch(x_gan, y_gan)\n",
    "# evaluate the model every n_eval epochs\n",
    "  if (i+1) % n_eval == 0:\n",
    "   summarize_performance(i, g_model, d_model, latent_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ff5d07b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999 0.6399999856948853 0.41999998688697815\n",
      "3999 0.41999998688697815 0.6100000143051147\n",
      "5999 0.7599999904632568 0.4399999976158142\n",
      "7999 0.699999988079071 0.3799999952316284\n",
      "9999 0.5199999809265137 0.47999998927116394\n"
     ]
    }
   ],
   "source": [
    "# train a generative adversarial network on a one-dimensional function\n",
    "import tensorflow as tf\n",
    "from numpy import hstack\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import rand\n",
    "from numpy.random import randn\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "# define the standalone discriminator model\n",
    "def define_discriminator(n_inputs=2):\n",
    " model = Sequential()\n",
    " model.add(Dense(25, activation='relu', kernel_initializer='he_uniform',\n",
    "   input_dim=n_inputs))\n",
    " model.add(Dense(1, activation='sigmoid'))\n",
    "# compile model\n",
    " model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    " return model\n",
    "# define the standalone generator model\n",
    "def define_generator(latent_dim, n_outputs=2):\n",
    " model = Sequential()\n",
    " model.add(Dense(15, activation='relu', kernel_initializer='he_uniform',\n",
    " input_dim=latent_dim))\n",
    " model.add(Dense(n_outputs, activation='linear'))\n",
    " return model\n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(generator, discriminator):\n",
    "# make weights in the discriminator not trainable\n",
    " discriminator.trainable = False\n",
    "# connect them\n",
    " model = Sequential()\n",
    "# add generator\n",
    " model.add(generator)\n",
    "# add the discriminator\n",
    " model.add(discriminator)\n",
    "# compile model\n",
    " model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    " return model\n",
    "# generate n real samples with class labels\n",
    "def generate_real_samples(n):\n",
    "# generate inputs in [-0.5, 0.5]\n",
    " X1 = rand(n) - 0.5\n",
    "# generate outputs X^2\n",
    " X2 = X1 * X1\n",
    "# stack arrays\n",
    " X1 = X1.reshape(n, 1)\n",
    " X2 = X2.reshape(n, 1)\n",
    " X = hstack((X1, X2))\n",
    "# generate class labels\n",
    " y = ones((n, 1))\n",
    " return X, y\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n):\n",
    "# generate points in the latent space\n",
    " x_input = randn(latent_dim * n)\n",
    "# reshape into a batch of inputs for the network\n",
    " x_input = x_input.reshape(n, latent_dim)\n",
    " return x_input\n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n):\n",
    "# generate points in latent space\n",
    " x_input = generate_latent_points(latent_dim, n)\n",
    "# predict outputs\n",
    " X = generator.predict(x_input)\n",
    "# create class labels\n",
    " y = zeros((n, 1))\n",
    " return X, y\n",
    "# evaluate the discriminator and plot real and fake points\n",
    "def summarize_performance(epoch, generator, discriminator, latent_dim, n=100):\n",
    "# prepare real samples\n",
    " x_real, y_real = generate_real_samples(n)\n",
    "# evaluate discriminator on real examples\n",
    " _, acc_real = discriminator.evaluate(x_real, y_real, verbose=0)\n",
    "# prepare fake examples\n",
    " x_fake, y_fake = generate_fake_samples(generator, latent_dim, n)\n",
    "# evaluate discriminator on fake examples\n",
    " _, acc_fake = discriminator.evaluate(x_fake, y_fake, verbose=0)\n",
    "# summarize discriminator performance\n",
    " print(epoch, acc_real, acc_fake)\n",
    "    \n",
    "# scatter plot real and fake data points\n",
    " pyplot.scatter(x_real[:, 0], x_real[:, 1], color='red')\n",
    " pyplot.scatter(x_fake[:, 0], x_fake[:, 1], color='blue')\n",
    "# save plot to file\n",
    " filename = 'generated_plot_e%03d.png' % (epoch+1)\n",
    " pyplot.savefig(filename)\n",
    " pyplot.close()\n",
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, latent_dim, n_epochs=10000, n_batch=128,\n",
    "n_eval=2000):\n",
    "# determine half the size of one batch, for updating the discriminator\n",
    " half_batch = int(n_batch / 2)\n",
    "# manually enumerate epochs\n",
    " for i in range(n_epochs):\n",
    "# prepare real samples\n",
    "  x_real, y_real = generate_real_samples(half_batch)\n",
    "# prepare fake examples\n",
    "  x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "# update discriminator\n",
    "  d_model.train_on_batch(x_real, y_real)\n",
    "  d_model.train_on_batch(x_fake, y_fake)\n",
    "# prepare points in latent space as input for the generator\n",
    "  x_gan = generate_latent_points(latent_dim, n_batch)\n",
    "# create inverted labels for the fake samples\n",
    "  y_gan = ones((n_batch, 1))\n",
    "# update the generator via the discriminators error\n",
    "  gan_model.train_on_batch(x_gan, y_gan)\n",
    "# evaluate the model every n_eval epochs\n",
    "  if (i+1) % n_eval == 0:\n",
    "   summarize_performance(i, g_model, d_model, latent_dim)\n",
    "# size of the latent space\n",
    "latent_dim = 5\n",
    "# create the discriminator\n",
    "discriminator = define_discriminator()\n",
    "# create the generator\n",
    "generator = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "# train model\n",
    "train(generator, discriminator, gan_model, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786193f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a09b49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
